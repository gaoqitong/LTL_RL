{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from env_current import *\n",
    "from collections import deque\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, buffer_size, random_seed = 123):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "        self.buffer = deque()\n",
    "        random.seed(random_seed)\n",
    "\n",
    "    def add(self, s, a, r, t, s2):\n",
    "        experience = (s, a, r, t, s2)\n",
    "        if self.count < self.buffer_size: \n",
    "            self.buffer.append(experience)\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "    def size(self):\n",
    "        return self.count\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "\n",
    "        batch = []\n",
    "\n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        s_batch = np.array([_[0] for _ in batch])\n",
    "        a_batch = np.array([_[1] for _ in batch])\n",
    "        r_batch = np.array([_[2] for _ in batch])\n",
    "        t_batch = np.array([_[3] for _ in batch])\n",
    "        s2_batch = np.array([_[4] for _ in batch])\n",
    "\n",
    "        return s_batch, a_batch, r_batch, t_batch, s2_batch\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        self.count = 0\n",
    "        \n",
    "def build_summaries():\n",
    "    episode_reward = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Reward\", episode_reward)\n",
    "    episode_ave_max_q = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Qmax_Value\", episode_ave_max_q)\n",
    "    exploration_rate = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Exploration\", exploration_rate)\n",
    "\n",
    "    summary_vars = [episode_reward, episode_ave_max_q, exploration_rate]\n",
    "    summary_ops = tf.summary.merge_all()\n",
    "\n",
    "    return summary_ops, summary_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QNet(object):\n",
    "    def __init__(self, sess, state_dim, action_dim, learning_rate, tau, batch_size, save_path):\n",
    "        self.sess = sess\n",
    "        self.learning_rate = learning_rate\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.save_path = save_path\n",
    "        \n",
    "        self.inputs, self.q_values, self.a_predict = self.build_net()\n",
    "        self.net_params = tf.trainable_variables()\n",
    "        \n",
    "        self.target_inputs, self.target_q_values, self.target_a_predict = self.build_net()\n",
    "        self.target_net_params = tf.trainable_variables()[len(self.net_params):]\n",
    "        \n",
    "        self.update_target_net_params = [self.target_net_params[i]\n",
    "                                         .assign(tf.multiply(self.tau, self.net_params[i])\n",
    "                                                 + tf.multiply((1.-self.tau), self.target_net_params[i]) ) \n",
    "                                         for i in range(len(self.target_net_params))]\n",
    "        \n",
    "        self.true_q_value = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "        self.action = tf.placeholder(shape=[None, 1], dtype=tf.int32)\n",
    "        \n",
    "        gather_indices = tf.range(MINIBATCH_SIZE) * tf.shape(self.q_values)[1] + tf.reshape(self.action, [-1])\n",
    "        self.action_correlated_q = tf.gather(tf.reshape(self.q_values,[-1]), gather_indices)\n",
    "        \n",
    "        self.loss = tf.losses.mean_squared_error(tf.reshape(self.true_q_value, [-1]), self.action_correlated_q)\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "        self.last_num_epi = -1\n",
    "        \n",
    "    def build_net(self):\n",
    "        s_inputs = tf.placeholder(shape = [None, self.state_dim], dtype = tf.float32)\n",
    "        W1 = tf.Variable(tf.random_uniform([self.state_dim, 400], 0, 0.1))\n",
    "        B1 = tf.Variable(tf.zeros([400]))\n",
    "        L1 = tf.add(tf.matmul(s_inputs, W1), B1)\n",
    "        L1 = tf.layers.batch_normalization(L1)\n",
    "        L1 = tf.nn.relu(L1)\n",
    "        W2 = tf.Variable(tf.random_uniform([400, 300], 0, 0.1))\n",
    "        B2 = tf.Variable(tf.zeros([300]))\n",
    "        L2 = tf.add(tf.matmul(L1, W2), B2)\n",
    "        L2 = tf.layers.batch_normalization(L2)\n",
    "        L2 = tf.nn.relu(L2)\n",
    "        W3 = tf.Variable(tf.random_uniform([300, self.action_dim], 0, 0.01))\n",
    "#         B3 = tf.Variable(tf.random_uniform([self.action_dim], -0.003, 0.003))\n",
    "#         q_values = tf.add(tf.matmul(L2, W3), B3)\n",
    "        q_values = tf.matmul(L2, W3)  \n",
    "        a_predict = tf.argmax(q_values,1)\n",
    "        \n",
    "        regularizer = tf.contrib.layers.l2_regularizer(0.01)\n",
    "        tf.contrib.layers.apply_regularization(regularizer,[W1, B1, W2, B2, W3])\n",
    "        return s_inputs, q_values, a_predict\n",
    "    \n",
    "    def train(self, states, action, true_q, num_epi):\n",
    "        if num_epi%20 == 0 and num_epi!=self.last_num_epi:\n",
    "            self.saver.save(self.sess, self.save_path)\n",
    "            print \"DDQN Saved\"\n",
    "            self.last_num_epi = num_epi\n",
    "            \n",
    "        return self.sess.run([self.q_values, self.optimizer], \n",
    "                             feed_dict={self.inputs: states, self.true_q_value: true_q, self.action: action})\n",
    "    \n",
    "    def predict_q(self, states):\n",
    "        return self.sess.run(self.q_values, feed_dict={self.inputs: states})\n",
    "    \n",
    "    def predict_a(self, states):\n",
    "        return self.sess.run(self.a_predict, feed_dict={self.inputs: states})\n",
    "    \n",
    "    def predect_target(self, states):\n",
    "        return self.sess.run(self.target_q_values, feed_dict={self.target_inputs: states})\n",
    "    \n",
    "    def update_target(self):\n",
    "        self.sess.run(self.update_target_net_params)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(sess, env, qnet):\n",
    "    \n",
    "    global EXPLORATION_RATE\n",
    "  \n",
    "    summary_ops, summary_vars = build_summaries()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(SUMMARY_DIR, sess.graph)\n",
    "    \n",
    "    qnet.update_target()\n",
    "    \n",
    "    replay_buffer = ReplayBuffer(BUFFER_SIZE, RANDOM_SEED)\n",
    "    \n",
    "    for num_epi in range(MAX_EPISODES):\n",
    "\n",
    "        s = env.reset()\n",
    "        s = [list(np.unravel_index(s, env.shape))]\n",
    "\n",
    "        ep_reward = 0\n",
    "        ep_ave_max_q = 0\n",
    "\n",
    "        for j in range(MAX_EPISODE_LEN):\n",
    "\n",
    "            a = np.argmax(qnet.predict_q(np.reshape(s, (1, qnet.state_dim))))\n",
    "    \n",
    "            if np.random.rand(1) < EXPLORATION_RATE:\n",
    "                s2, r, terminal, info = env.step(np.random.randint(0,qnet.action_dim))\n",
    "            else:\n",
    "                s2, r, terminal, info = env.step(a)\n",
    "            \n",
    "            s2 = list(np.unravel_index(s2, env.shape))\n",
    "\n",
    "            replay_buffer.add(np.reshape(s, (qnet.state_dim,)), np.reshape(a, (1,)), r,\n",
    "                              terminal, np.reshape(s2, (qnet.state_dim,)))\n",
    "\n",
    "            # Keep adding experience to the memory until\n",
    "            # there are at least minibatch size samples\n",
    "            if replay_buffer.size() > MINIBATCH_SIZE:\n",
    "                s_batch, a_batch, r_batch, t_batch, s2_batch = replay_buffer.sample_batch(MINIBATCH_SIZE)\n",
    "\n",
    "                # Calculate targets\n",
    "                target_q = qnet.predect_target(s2_batch)\n",
    "\n",
    "                y_i = []\n",
    "                for k in range(MINIBATCH_SIZE):\n",
    "                    if t_batch[k]:\n",
    "                        y_i.append(r_batch[k])\n",
    "                    else:\n",
    "                        y_i.append(r_batch[k] + GAMMA * np.amax(target_q[k]))\n",
    "\n",
    "                # Update the critic given the targets\n",
    "                predicted_q_value, _ = qnet.train(s_batch, a_batch, np.reshape(y_i, (MINIBATCH_SIZE, 1)), num_epi)\n",
    "\n",
    "                ep_ave_max_q += np.amax(predicted_q_value)\n",
    "                \n",
    "                # Update target networks\n",
    "                qnet.update_target()\n",
    "\n",
    "            s = s2\n",
    "            ep_reward += r\n",
    "\n",
    "            if terminal or j == MAX_EPISODE_LEN-1:\n",
    "                \n",
    "                if EXPLORATION_RATE > 0.05 and terminal:\n",
    "                    EXPLORATION_RATE = EXPLORATION_RATE*0.92\n",
    "\n",
    "                summary_str = sess.run(summary_ops, feed_dict={\n",
    "                    summary_vars[0]: ep_reward,\n",
    "                    summary_vars[1]: ep_ave_max_q / float(j),\n",
    "                    summary_vars[2]: EXPLORATION_RATE\n",
    "                })\n",
    "\n",
    "                writer.add_summary(summary_str, num_epi)\n",
    "                writer.flush()\n",
    "\n",
    "                print('| Reward: {:d} | Episode: {:d} | Qmax: {:.4f} | Exploration: {:.6f} '.format(int(ep_reward), \\\n",
    "                        num_epi, (ep_ave_max_q / float(j)), EXPLORATION_RATE))\n",
    "                \n",
    "                f = open(\"stats.txt\", \"ab\")\n",
    "                f.write(\"| Reward: \" + str(int(ep_reward)) \n",
    "                        +\" | Episode: \" + str(num_epi) \n",
    "                        + \" | Qmax: \" + str(ep_ave_max_q / float(j)) \n",
    "                        + \" | Exploration: \" + str(EXPLORATION_RATE) + \"\\n\")\n",
    "                f.close()\n",
    "                \n",
    "                break\n",
    "                \n",
    "        if num_epi%1 == 0:\n",
    "            state_list = []\n",
    "            action_list = []\n",
    "            world = np.zeros(env.shape)\n",
    "            for state in range(env.nS):\n",
    "                state = np.unravel_index(state, env.shape)\n",
    "                action = qnet.predict_q(np.reshape(state, (1,state_dim)))\n",
    "                action = np.argmax(action)\n",
    "                state_list.append(state)\n",
    "                action_list.append(action)\n",
    "                \n",
    "#             print np.reshape(action_list, env.shape)\n",
    "                \n",
    "            f = open(\"action.txt\",\"ab\")\n",
    "            np.savetxt(f, np.reshape(action_list, env.shape), fmt=\"%i\")\n",
    "            f.write(\"---------------------------\\n\")\n",
    "            f.close()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0015\n",
    "GAMMA = 0.99\n",
    "TAU = 0.001\n",
    "BUFFER_SIZE = 10**6\n",
    "MINIBATCH_SIZE = 64\n",
    "RANDOM_SEED = 272\n",
    "MAX_EPISODES = 50000\n",
    "MAX_EPISODE_LEN = 1000\n",
    "SUMMARY_DIR = './results/tf_ddqn'\n",
    "EXPLORATION_RATE = 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gaoqitong/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDQN Saved\n",
      "| Reward: -1000 | Episode: 0 | Qmax: 52.1282 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 1 | Qmax: 63.0277 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 2 | Qmax: 64.3090 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 3 | Qmax: 63.2443 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 4 | Qmax: 61.5773 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 5 | Qmax: 59.6755 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 6 | Qmax: 57.6404 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 7 | Qmax: 55.6209 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 8 | Qmax: 53.5161 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 9 | Qmax: 51.5566 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 10 | Qmax: 49.7000 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 11 | Qmax: 47.8952 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 12 | Qmax: 46.0578 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 13 | Qmax: 44.3536 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 14 | Qmax: 42.7597 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 15 | Qmax: 41.1040 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 16 | Qmax: 39.5382 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 17 | Qmax: 37.9979 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 18 | Qmax: 36.4155 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 19 | Qmax: 34.9156 | Exploration: 0.650000 \n",
      "DDQN Saved\n",
      "| Reward: -1000 | Episode: 20 | Qmax: 33.4395 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 21 | Qmax: 31.9749 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 22 | Qmax: 30.5445 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 23 | Qmax: 29.0904 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 24 | Qmax: 27.6756 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 25 | Qmax: 26.2851 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 26 | Qmax: 24.9086 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 27 | Qmax: 23.5688 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 28 | Qmax: 22.2172 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 29 | Qmax: 20.8919 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 30 | Qmax: 19.5710 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 31 | Qmax: 18.2294 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 32 | Qmax: 16.8424 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 33 | Qmax: 15.4834 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 34 | Qmax: 14.2170 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 35 | Qmax: 12.9597 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 36 | Qmax: 11.7082 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 37 | Qmax: 10.4820 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 38 | Qmax: 9.2573 | Exploration: 0.650000 \n",
      "| Reward: -1000 | Episode: 39 | Qmax: 8.0568 | Exploration: 0.650000 \n",
      "DDQN Saved\n",
      "| Reward: -1000 | Episode: 40 | Qmax: 6.8662 | Exploration: 0.650000 \n",
      "| Reward: -33 | Episode: 41 | Qmax: 6.4525 | Exploration: 0.598000 \n",
      "| Reward: -1000 | Episode: 42 | Qmax: 5.6515 | Exploration: 0.598000 \n",
      "| Reward: -1000 | Episode: 43 | Qmax: 4.4902 | Exploration: 0.598000 \n",
      "| Reward: -1000 | Episode: 44 | Qmax: 3.3453 | Exploration: 0.598000 \n",
      "| Reward: -1000 | Episode: 45 | Qmax: 2.2077 | Exploration: 0.598000 \n",
      "| Reward: -1000 | Episode: 46 | Qmax: 1.0870 | Exploration: 0.598000 \n",
      "| Reward: -55 | Episode: 47 | Qmax: 0.4880 | Exploration: 0.550160 \n",
      "| Reward: -1000 | Episode: 48 | Qmax: -0.1315 | Exploration: 0.550160 \n",
      "| Reward: -1000 | Episode: 49 | Qmax: -1.2518 | Exploration: 0.550160 \n",
      "| Reward: -1000 | Episode: 50 | Qmax: -2.2690 | Exploration: 0.550160 \n",
      "| Reward: -1000 | Episode: 51 | Qmax: -3.2657 | Exploration: 0.550160 \n",
      "| Reward: -95 | Episode: 52 | Qmax: -3.8309 | Exploration: 0.506147 \n",
      "| Reward: -287 | Episode: 53 | Qmax: -3.9768 | Exploration: 0.465655 \n",
      "| Reward: -1000 | Episode: 54 | Qmax: -4.5311 | Exploration: 0.465655 \n",
      "| Reward: -577 | Episode: 55 | Qmax: -5.2111 | Exploration: 0.428403 \n",
      "| Reward: -582 | Episode: 56 | Qmax: -5.6671 | Exploration: 0.394131 \n",
      "| Reward: -1000 | Episode: 57 | Qmax: -6.3391 | Exploration: 0.394131 \n",
      "| Reward: -524 | Episode: 58 | Qmax: -6.9397 | Exploration: 0.362600 \n",
      "| Reward: -1000 | Episode: 59 | Qmax: -7.5763 | Exploration: 0.362600 \n",
      "DDQN Saved\n",
      "| Reward: -75 | Episode: 60 | Qmax: -8.1256 | Exploration: 0.333592 \n",
      "| Reward: -132 | Episode: 61 | Qmax: -8.1481 | Exploration: 0.306905 \n",
      "| Reward: -1000 | Episode: 62 | Qmax: -8.5250 | Exploration: 0.306905 \n",
      "| Reward: -280 | Episode: 63 | Qmax: -9.0937 | Exploration: 0.282352 \n",
      "| Reward: -403 | Episode: 64 | Qmax: -9.3822 | Exploration: 0.259764 \n",
      "| Reward: -1000 | Episode: 65 | Qmax: -9.9354 | Exploration: 0.259764 \n",
      "| Reward: -625 | Episode: 66 | Qmax: -10.5557 | Exploration: 0.238983 \n",
      "| Reward: -1000 | Episode: 67 | Qmax: -11.2185 | Exploration: 0.238983 \n",
      "| Reward: -899 | Episode: 68 | Qmax: -11.9613 | Exploration: 0.219864 \n",
      "| Reward: -126 | Episode: 69 | Qmax: -12.3394 | Exploration: 0.202275 \n",
      "| Reward: -1000 | Episode: 70 | Qmax: -12.7247 | Exploration: 0.202275 \n",
      "| Reward: -132 | Episode: 71 | Qmax: -13.1774 | Exploration: 0.186093 \n",
      "| Reward: -712 | Episode: 72 | Qmax: -13.4592 | Exploration: 0.171206 \n",
      "| Reward: -275 | Episode: 73 | Qmax: -13.8235 | Exploration: 0.157509 \n",
      "| Reward: -578 | Episode: 74 | Qmax: -14.1592 | Exploration: 0.144909 \n",
      "| Reward: -75 | Episode: 75 | Qmax: -14.6529 | Exploration: 0.133316 \n",
      "| Reward: -445 | Episode: 76 | Qmax: -14.6556 | Exploration: 0.122651 \n",
      "| Reward: -95 | Episode: 77 | Qmax: -14.9735 | Exploration: 0.112839 \n",
      "| Reward: -25 | Episode: 78 | Qmax: -15.4715 | Exploration: 0.103812 \n",
      "| Reward: -51 | Episode: 79 | Qmax: -15.2514 | Exploration: 0.095507 \n",
      "DDQN Saved\n",
      "| Reward: -45 | Episode: 80 | Qmax: -15.3231 | Exploration: 0.087866 \n",
      "| Reward: -47 | Episode: 81 | Qmax: -15.3088 | Exploration: 0.080837 \n",
      "| Reward: -43 | Episode: 82 | Qmax: -15.3789 | Exploration: 0.074370 \n",
      "| Reward: -51 | Episode: 83 | Qmax: -15.4005 | Exploration: 0.068420 \n",
      "| Reward: -21 | Episode: 84 | Qmax: -15.8330 | Exploration: 0.062947 \n",
      "| Reward: -49 | Episode: 85 | Qmax: -15.4502 | Exploration: 0.057911 \n",
      "| Reward: -19 | Episode: 86 | Qmax: -15.9743 | Exploration: 0.053278 \n",
      "| Reward: -232 | Episode: 87 | Qmax: -15.1741 | Exploration: 0.049016 \n",
      "| Reward: -1000 | Episode: 88 | Qmax: -15.4911 | Exploration: 0.049016 \n",
      "| Reward: -511 | Episode: 89 | Qmax: -15.9892 | Exploration: 0.049016 \n",
      "| Reward: -593 | Episode: 90 | Qmax: -16.2383 | Exploration: 0.049016 \n",
      "| Reward: -397 | Episode: 91 | Qmax: -16.6783 | Exploration: 0.049016 \n",
      "| Reward: -322 | Episode: 92 | Qmax: -16.8970 | Exploration: 0.049016 \n",
      "| Reward: -280 | Episode: 93 | Qmax: -17.1941 | Exploration: 0.049016 \n",
      "| Reward: -130 | Episode: 94 | Qmax: -17.4583 | Exploration: 0.049016 \n",
      "| Reward: -70 | Episode: 95 | Qmax: -17.3781 | Exploration: 0.049016 \n",
      "| Reward: -42 | Episode: 96 | Qmax: -17.6942 | Exploration: 0.049016 \n",
      "| Reward: -86 | Episode: 97 | Qmax: -17.5609 | Exploration: 0.049016 \n",
      "| Reward: -204 | Episode: 98 | Qmax: -17.4360 | Exploration: 0.049016 \n",
      "| Reward: -142 | Episode: 99 | Qmax: -17.5518 | Exploration: 0.049016 \n",
      "DDQN Saved\n",
      "| Reward: -76 | Episode: 100 | Qmax: -17.7193 | Exploration: 0.049016 \n",
      "| Reward: -332 | Episode: 101 | Qmax: -17.7538 | Exploration: 0.049016 \n",
      "| Reward: -70 | Episode: 102 | Qmax: -18.0279 | Exploration: 0.049016 \n",
      "| Reward: -42 | Episode: 103 | Qmax: -18.1950 | Exploration: 0.049016 \n",
      "| Reward: -318 | Episode: 104 | Qmax: -17.9624 | Exploration: 0.049016 \n",
      "| Reward: -87 | Episode: 105 | Qmax: -18.2879 | Exploration: 0.049016 \n",
      "| Reward: -59 | Episode: 106 | Qmax: -18.4863 | Exploration: 0.049016 \n",
      "| Reward: -91 | Episode: 107 | Qmax: -18.4146 | Exploration: 0.049016 \n",
      "| Reward: -174 | Episode: 108 | Qmax: -18.2783 | Exploration: 0.049016 \n",
      "| Reward: -119 | Episode: 109 | Qmax: -18.4084 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 110 | Qmax: -18.9995 | Exploration: 0.049016 \n",
      "| Reward: -92 | Episode: 111 | Qmax: -18.4743 | Exploration: 0.049016 \n",
      "| Reward: -250 | Episode: 112 | Qmax: -18.4864 | Exploration: 0.049016 \n",
      "| Reward: -87 | Episode: 113 | Qmax: -18.8459 | Exploration: 0.049016 \n",
      "| Reward: -571 | Episode: 114 | Qmax: -18.7553 | Exploration: 0.049016 \n",
      "| Reward: -27 | Episode: 115 | Qmax: -19.6624 | Exploration: 0.049016 \n",
      "| Reward: -279 | Episode: 116 | Qmax: -19.0050 | Exploration: 0.049016 \n",
      "| Reward: -123 | Episode: 117 | Qmax: -19.3178 | Exploration: 0.049016 \n",
      "| Reward: -45 | Episode: 118 | Qmax: -19.5623 | Exploration: 0.049016 \n",
      "| Reward: -280 | Episode: 119 | Qmax: -19.2366 | Exploration: 0.049016 \n",
      "DDQN Saved\n",
      "| Reward: -55 | Episode: 120 | Qmax: -19.4876 | Exploration: 0.049016 \n",
      "| Reward: -144 | Episode: 121 | Qmax: -19.5064 | Exploration: 0.049016 \n",
      "| Reward: -25 | Episode: 122 | Qmax: -20.3492 | Exploration: 0.049016 \n",
      "| Reward: -77 | Episode: 123 | Qmax: -19.7077 | Exploration: 0.049016 \n",
      "| Reward: -427 | Episode: 124 | Qmax: -19.5473 | Exploration: 0.049016 \n",
      "| Reward: -589 | Episode: 125 | Qmax: -19.6765 | Exploration: 0.049016 \n",
      "| Reward: -116 | Episode: 126 | Qmax: -19.9568 | Exploration: 0.049016 \n",
      "| Reward: -27 | Episode: 127 | Qmax: -20.7334 | Exploration: 0.049016 \n",
      "| Reward: -43 | Episode: 128 | Qmax: -20.6534 | Exploration: 0.049016 \n",
      "| Reward: -21 | Episode: 129 | Qmax: -21.0072 | Exploration: 0.049016 \n",
      "| Reward: -121 | Episode: 130 | Qmax: -20.1039 | Exploration: 0.049016 \n",
      "| Reward: -127 | Episode: 131 | Qmax: -20.1510 | Exploration: 0.049016 \n",
      "| Reward: -180 | Episode: 132 | Qmax: -20.1031 | Exploration: 0.049016 \n",
      "| Reward: -705 | Episode: 133 | Qmax: -20.2002 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 134 | Qmax: -20.9134 | Exploration: 0.049016 \n",
      "| Reward: -107 | Episode: 135 | Qmax: -20.8073 | Exploration: 0.049016 \n",
      "| Reward: -71 | Episode: 136 | Qmax: -20.7977 | Exploration: 0.049016 \n",
      "| Reward: -29 | Episode: 137 | Qmax: -21.0458 | Exploration: 0.049016 \n",
      "| Reward: -29 | Episode: 138 | Qmax: -20.9407 | Exploration: 0.049016 \n",
      "| Reward: -19 | Episode: 139 | Qmax: -21.7138 | Exploration: 0.049016 \n",
      "DDQN Saved\n",
      "| Reward: -53 | Episode: 140 | Qmax: -20.9557 | Exploration: 0.049016 \n",
      "| Reward: -143 | Episode: 141 | Qmax: -20.6360 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 142 | Qmax: -21.2139 | Exploration: 0.049016 \n",
      "| Reward: -85 | Episode: 143 | Qmax: -20.6133 | Exploration: 0.049016 \n",
      "| Reward: -97 | Episode: 144 | Qmax: -20.7613 | Exploration: 0.049016 \n",
      "| Reward: -19 | Episode: 145 | Qmax: -21.7322 | Exploration: 0.049016 \n",
      "| Reward: -25 | Episode: 146 | Qmax: -21.2380 | Exploration: 0.049016 \n",
      "| Reward: -19 | Episode: 147 | Qmax: -21.7124 | Exploration: 0.049016 \n",
      "| Reward: -33 | Episode: 148 | Qmax: -21.1234 | Exploration: 0.049016 \n",
      "| Reward: -35 | Episode: 149 | Qmax: -21.2692 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 150 | Qmax: -22.0329 | Exploration: 0.049016 \n",
      "| Reward: -19 | Episode: 151 | Qmax: -21.9788 | Exploration: 0.049016 \n",
      "| Reward: -23 | Episode: 152 | Qmax: -21.5831 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 153 | Qmax: -22.1981 | Exploration: 0.049016 \n",
      "| Reward: -148 | Episode: 154 | Qmax: -20.7218 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 155 | Qmax: -22.1674 | Exploration: 0.049016 \n",
      "| Reward: -55 | Episode: 156 | Qmax: -21.0419 | Exploration: 0.049016 \n",
      "| Reward: -39 | Episode: 157 | Qmax: -21.1574 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 158 | Qmax: -21.7766 | Exploration: 0.049016 \n",
      "| Reward: -45 | Episode: 159 | Qmax: -21.1051 | Exploration: 0.049016 \n",
      "DDQN Saved\n",
      "| Reward: -25 | Episode: 160 | Qmax: -21.6187 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 161 | Qmax: -22.0175 | Exploration: 0.049016 \n",
      "| Reward: -45 | Episode: 162 | Qmax: -21.2061 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 163 | Qmax: -21.2791 | Exploration: 0.049016 \n",
      "| Reward: -17 | Episode: 164 | Qmax: -21.8743 | Exploration: 0.049016 \n",
      "| Reward: -127 | Episode: 165 | Qmax: -20.6208 | Exploration: 0.049016 \n",
      "| Reward: -17 | Episode: 166 | Qmax: -21.9271 | Exploration: 0.049016 \n",
      "| Reward: -25 | Episode: 167 | Qmax: -21.6838 | Exploration: 0.049016 \n",
      "| Reward: -23 | Episode: 168 | Qmax: -21.8013 | Exploration: 0.049016 \n",
      "| Reward: -21 | Episode: 169 | Qmax: -21.9145 | Exploration: 0.049016 \n",
      "| Reward: -37 | Episode: 170 | Qmax: -21.4136 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 171 | Qmax: -22.7305 | Exploration: 0.049016 \n",
      "| Reward: -21 | Episode: 172 | Qmax: -21.6718 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 173 | Qmax: -21.9479 | Exploration: 0.049016 \n",
      "| Reward: -19 | Episode: 174 | Qmax: -21.7090 | Exploration: 0.049016 \n",
      "| Reward: -21 | Episode: 175 | Qmax: -21.3228 | Exploration: 0.049016 \n",
      "| Reward: -17 | Episode: 176 | Qmax: -22.0501 | Exploration: 0.049016 \n",
      "| Reward: -27 | Episode: 177 | Qmax: -21.7898 | Exploration: 0.049016 \n",
      "| Reward: -31 | Episode: 178 | Qmax: -21.2913 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 179 | Qmax: -21.9104 | Exploration: 0.049016 \n",
      "DDQN Saved\n",
      "| Reward: -35 | Episode: 180 | Qmax: -21.3811 | Exploration: 0.049016 \n",
      "| Reward: -132 | Episode: 181 | Qmax: -20.8349 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 182 | Qmax: -22.3053 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 183 | Qmax: -22.4133 | Exploration: 0.049016 \n",
      "| Reward: -17 | Episode: 184 | Qmax: -21.8424 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 185 | Qmax: -21.9583 | Exploration: 0.049016 \n",
      "| Reward: -17 | Episode: 186 | Qmax: -21.7933 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 187 | Qmax: -22.2440 | Exploration: 0.049016 \n",
      "| Reward: -23 | Episode: 188 | Qmax: -21.7420 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 189 | Qmax: -22.2365 | Exploration: 0.049016 \n",
      "| Reward: -17 | Episode: 190 | Qmax: -21.7058 | Exploration: 0.049016 \n",
      "| Reward: -33 | Episode: 191 | Qmax: -21.3263 | Exploration: 0.049016 \n",
      "| Reward: -19 | Episode: 192 | Qmax: -21.9992 | Exploration: 0.049016 \n",
      "| Reward: -223 | Episode: 193 | Qmax: -20.6601 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 194 | Qmax: -21.8765 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 195 | Qmax: -21.7788 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 196 | Qmax: -22.1044 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 197 | Qmax: -22.2251 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 198 | Qmax: -22.1942 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 199 | Qmax: -22.3742 | Exploration: 0.049016 \n",
      "DDQN Saved\n",
      "| Reward: -15 | Episode: 200 | Qmax: -22.0970 | Exploration: 0.049016 \n",
      "| Reward: -31 | Episode: 201 | Qmax: -21.4086 | Exploration: 0.049016 \n",
      "| Reward: -97 | Episode: 202 | Qmax: -20.8412 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 203 | Qmax: -22.1721 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 204 | Qmax: -22.0470 | Exploration: 0.049016 \n",
      "| Reward: -49 | Episode: 205 | Qmax: -21.2366 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 206 | Qmax: -22.2904 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 207 | Qmax: -21.6391 | Exploration: 0.049016 \n",
      "| Reward: -112 | Episode: 208 | Qmax: -20.8312 | Exploration: 0.049016 \n",
      "| Reward: -17 | Episode: 209 | Qmax: -22.2912 | Exploration: 0.049016 \n",
      "| Reward: -17 | Episode: 210 | Qmax: -22.2839 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 211 | Qmax: -22.0221 | Exploration: 0.049016 \n",
      "| Reward: -17 | Episode: 212 | Qmax: -21.7752 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 213 | Qmax: -22.1042 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 214 | Qmax: -21.9633 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 215 | Qmax: -22.0783 | Exploration: 0.049016 \n",
      "| Reward: -17 | Episode: 216 | Qmax: -21.9274 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 217 | Qmax: -22.0865 | Exploration: 0.049016 \n",
      "| Reward: -17 | Episode: 218 | Qmax: -21.3334 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 219 | Qmax: -21.5834 | Exploration: 0.049016 \n",
      "DDQN Saved\n",
      "| Reward: -15 | Episode: 220 | Qmax: -21.7409 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 221 | Qmax: -21.7967 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 222 | Qmax: -22.0177 | Exploration: 0.049016 \n",
      "| Reward: -19 | Episode: 223 | Qmax: -21.7581 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 224 | Qmax: -21.4708 | Exploration: 0.049016 \n",
      "| Reward: -17 | Episode: 225 | Qmax: -21.5873 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 226 | Qmax: -21.7944 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 227 | Qmax: -22.1058 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 228 | Qmax: -22.0138 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 229 | Qmax: -21.9369 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 230 | Qmax: -21.6394 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 231 | Qmax: -21.3049 | Exploration: 0.049016 \n",
      "| Reward: -25 | Episode: 232 | Qmax: -20.9914 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 233 | Qmax: -21.0756 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 234 | Qmax: -21.9436 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 235 | Qmax: -22.1729 | Exploration: 0.049016 \n",
      "| Reward: -19 | Episode: 236 | Qmax: -21.5913 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 237 | Qmax: -21.6744 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 238 | Qmax: -21.6263 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 239 | Qmax: -21.8007 | Exploration: 0.049016 \n",
      "DDQN Saved\n",
      "| Reward: -17 | Episode: 240 | Qmax: -21.4878 | Exploration: 0.049016 \n",
      "| Reward: -17 | Episode: 241 | Qmax: -21.8970 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 242 | Qmax: -21.3904 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 243 | Qmax: -21.3730 | Exploration: 0.049016 \n",
      "| Reward: -19 | Episode: 244 | Qmax: -20.8100 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 245 | Qmax: -20.9924 | Exploration: 0.049016 \n",
      "| Reward: -17 | Episode: 246 | Qmax: -21.3430 | Exploration: 0.049016 \n",
      "| Reward: -17 | Episode: 247 | Qmax: -21.5767 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 248 | Qmax: -21.2692 | Exploration: 0.049016 \n",
      "| Reward: -19 | Episode: 249 | Qmax: -21.0419 | Exploration: 0.049016 \n",
      "| Reward: -25 | Episode: 250 | Qmax: -21.0219 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 251 | Qmax: -21.7973 | Exploration: 0.049016 \n",
      "| Reward: -17 | Episode: 252 | Qmax: -20.9567 | Exploration: 0.049016 \n",
      "| Reward: -21 | Episode: 253 | Qmax: -20.6624 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 254 | Qmax: -21.6137 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 255 | Qmax: -21.1202 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 256 | Qmax: -21.1589 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 257 | Qmax: -21.1751 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 258 | Qmax: -20.8912 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 259 | Qmax: -20.6679 | Exploration: 0.049016 \n",
      "DDQN Saved\n",
      "| Reward: -15 | Episode: 260 | Qmax: -21.0311 | Exploration: 0.049016 \n",
      "| Reward: -25 | Episode: 261 | Qmax: -21.0852 | Exploration: 0.049016 \n",
      "| Reward: -17 | Episode: 262 | Qmax: -21.1251 | Exploration: 0.049016 \n",
      "| Reward: -15 | Episode: 263 | Qmax: -21.0792 | Exploration: 0.049016 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e28ba9506122>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mQnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTAU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMINIBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./saved_model/ddqn.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-12cdcd95f0b2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(sess, env, qnet)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munravel_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0mstate_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-70cb78b3fd25>\u001b[0m in \u001b[0;36mpredict_q\u001b[0;34m(self, states)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gaoqitong/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gaoqitong/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gaoqitong/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/gaoqitong/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gaoqitong/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    env = CurrentWorld()\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    env.seed(RANDOM_SEED)\n",
    "    \n",
    "    state_dim = 2\n",
    "    action_dim = 4\n",
    "    \n",
    "    Qnet = QNet(sess, state_dim, action_dim, LEARNING_RATE, TAU, MINIBATCH_SIZE, \"./saved_model/ddqn.ckpt\")\n",
    "    \n",
    "    train(sess, env, Qnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "world = np.zeros(env.shape)\n",
    "a_list = []\n",
    "s_list = []\n",
    "for s in range(env.nS):\n",
    "    a_list += [np.argmax(P[s])]\n",
    "    s_list += [np.unravel_index(s,env.shape)]\n",
    "for s,a in zip(s_list,a_list):\n",
    "    world[s] = a\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%matplotlib auto\n",
    "plt.imshow(world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "plotting.plot_episode_stats(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def get_optimal_path(Q,env):\n",
    "    env.reset()\n",
    "    start_state = env.start_state\n",
    "    terminal_state = env.terminal_state\n",
    "    state = np.ravel_multi_index(start_state,env.shape)\n",
    "    path = [start_state]\n",
    "    value = 0\n",
    "    action = []\n",
    "    while 1:\n",
    "        next_action = np.argmax(Q[state])\n",
    "        next_state,reward,done,_ = env.step(next_action)\n",
    "        path += [np.unravel_index(next_state,env.shape)]\n",
    "        value += reward\n",
    "        action += [next_action]\n",
    "        if done:\n",
    "            return path, action, value\n",
    "            break\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "opt_path,action,value = get_optimal_path(Q,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "%matplotlib auto\n",
    "world = deepcopy(env.winds)\n",
    "t = 0\n",
    "for i in opt_path[:-1]:\n",
    "    world[i] = 6\n",
    "#     world[i] += action[t]\n",
    "    t+=1\n",
    "plt.imshow(world)\n",
    "# print value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

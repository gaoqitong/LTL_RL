{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from env_current import *\n",
    "from collections import deque\n",
    "import random\n",
    "import gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, buffer_size, random_seed = 123):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.count = 0\n",
    "        self.buffer = deque()\n",
    "        random.seed(random_seed)\n",
    "\n",
    "    def add(self, s, a, r, t, s2):\n",
    "        experience = (s, a, r, t, s2)\n",
    "        if self.count < self.buffer_size: \n",
    "            self.buffer.append(experience)\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.buffer.popleft()\n",
    "            self.buffer.append(experience)\n",
    "\n",
    "    def size(self):\n",
    "        return self.count\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "\n",
    "        batch = []\n",
    "\n",
    "        if self.count < batch_size:\n",
    "            batch = random.sample(self.buffer, self.count)\n",
    "        else:\n",
    "            batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        s_batch = np.array([_[0] for _ in batch])\n",
    "        a_batch = np.array([_[1] for _ in batch])\n",
    "        r_batch = np.array([_[2] for _ in batch])\n",
    "        t_batch = np.array([_[3] for _ in batch])\n",
    "        s2_batch = np.array([_[4] for _ in batch])\n",
    "\n",
    "        return s_batch, a_batch, r_batch, t_batch, s2_batch\n",
    "\n",
    "    def clear(self):\n",
    "        self.buffer.clear()\n",
    "        self.count = 0\n",
    "        \n",
    "def build_summaries():\n",
    "    episode_reward = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Reward\", episode_reward)\n",
    "    episode_ave_max_q = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Qmax_Value\", episode_ave_max_q)\n",
    "    exploration_rate = tf.Variable(0.)\n",
    "    tf.summary.scalar(\"Exploration\", exploration_rate)\n",
    "\n",
    "    summary_vars = [episode_reward, episode_ave_max_q, exploration_rate]\n",
    "    summary_ops = tf.summary.merge_all()\n",
    "\n",
    "    return summary_ops, summary_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QNet(object):\n",
    "    def __init__(self, sess, state_dim, action_dim, learning_rate, tau, batch_size, save_path):\n",
    "        self.sess = sess\n",
    "        self.learning_rate = learning_rate\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.save_path = save_path\n",
    "        \n",
    "        self.inputs, self.q_values, self.a_predict = self.build_net()\n",
    "        self.net_params = tf.trainable_variables()\n",
    "        \n",
    "        self.target_inputs, self.target_q_values, self.target_a_predict = self.build_net()\n",
    "        self.target_net_params = tf.trainable_variables()[len(self.net_params):]\n",
    "        \n",
    "        self.update_target_net_params = [self.target_net_params[i]\n",
    "                                         .assign(tf.multiply(self.tau, self.net_params[i])\n",
    "                                                 + tf.multiply((1.-self.tau), self.target_net_params[i]) ) \n",
    "                                         for i in range(len(self.target_net_params))]\n",
    "        \n",
    "        self.true_q_value = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
    "        self.action = tf.placeholder(shape=[None, 1], dtype=tf.int32)\n",
    "        \n",
    "        gather_indices = tf.range(MINIBATCH_SIZE) * tf.shape(self.q_values)[1] + tf.reshape(self.action, [-1])\n",
    "        self.action_correlated_q = tf.gather(tf.reshape(self.q_values,[-1]), gather_indices)\n",
    "        \n",
    "        self.loss = tf.losses.mean_squared_error(tf.reshape(self.true_q_value, [-1]), self.action_correlated_q)\n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "        self.last_num_epi = -1\n",
    "        \n",
    "    def build_net(self):\n",
    "        s_inputs = tf.placeholder(shape = [None, self.state_dim], dtype = tf.float32)\n",
    "        W1 = tf.Variable(tf.random_uniform([self.state_dim, 20], 0, 0.005))\n",
    "        B1 = tf.Variable(tf.zeros([20]))\n",
    "        L1 = tf.add(tf.matmul(s_inputs, W1), B1)\n",
    "        L1 = tf.layers.batch_normalization(L1)\n",
    "        L1 = tf.nn.relu(L1)\n",
    "        W2 = tf.Variable(tf.random_uniform([20, 10], 0, 0.005))\n",
    "        B2 = tf.Variable(tf.zeros([10]))\n",
    "        L2 = tf.add(tf.matmul(L1, W2), B2)\n",
    "        L2 = tf.layers.batch_normalization(L2)\n",
    "        L2 = tf.nn.relu(L2)\n",
    "        W3 = tf.Variable(tf.random_uniform([10, self.action_dim], 0, 0.001))\n",
    "#         B3 = tf.Variable(tf.random_uniform([self.action_dim], 0, 0.003))\n",
    "#         q_values = tf.add(tf.matmul(L2, W3), B3)\n",
    "        q_values = tf.matmul(L2, W3)\n",
    "        a_predict = tf.argmax(q_values,1)\n",
    "        return s_inputs, q_values, a_predict\n",
    "    \n",
    "    def train(self, states, action, true_q, num_epi):\n",
    "        if num_epi%100 == 0 and num_epi!=self.last_num_epi:\n",
    "            self.saver.save(self.sess, self.save_path)\n",
    "            print \"DDQN Saved\"\n",
    "            self.last_num_epi = num_epi\n",
    "            \n",
    "        return self.sess.run([self.q_values, self.optimizer], \n",
    "                             feed_dict={self.inputs: states, self.true_q_value: true_q, self.action: action})\n",
    "    \n",
    "    def predict_q(self, states):\n",
    "        return self.sess.run(self.q_values, feed_dict={self.inputs: states})\n",
    "    \n",
    "    def predict_a(self, states):\n",
    "        return self.sess.run(self.a_predict, feed_dict={self.inputs: states})\n",
    "    \n",
    "    def predict_target(self, states):\n",
    "        return self.sess.run(self.target_q_values, feed_dict={self.target_inputs: states})\n",
    "    \n",
    "    def update_target(self):\n",
    "        self.sess.run(self.update_target_net_params)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(sess, env, qnet):\n",
    "    \n",
    "    global EXPLORATION_RATE\n",
    "  \n",
    "    summary_ops, summary_vars = build_summaries()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter(SUMMARY_DIR, sess.graph)\n",
    "    \n",
    "    qnet.update_target()\n",
    "    \n",
    "    replay_buffer = ReplayBuffer(BUFFER_SIZE, RANDOM_SEED)\n",
    "    \n",
    "    for num_epi in range(MAX_EPISODES):\n",
    "\n",
    "        s = env.reset()\n",
    "\n",
    "        ep_reward = 0\n",
    "        ep_ave_max_q = 0\n",
    "\n",
    "        for j in range(MAX_EPISODE_LEN):\n",
    "            \n",
    "            env.render()\n",
    "            \n",
    "            \n",
    "            pred = qnet.predict_q(np.reshape(s, (1, qnet.state_dim)))\n",
    "            a = np.argmax(pred)\n",
    "            print pred\n",
    "    \n",
    "            if np.random.rand(1) < EXPLORATION_RATE:\n",
    "                s2, r, terminal, info = env.step(np.random.randint(0,qnet.action_dim))\n",
    "            else:\n",
    "                s2, r, terminal, info = env.step(a)\n",
    "        \n",
    "\n",
    "            replay_buffer.add(np.reshape(s, (qnet.state_dim,)), np.reshape(a, (1,)), r,\n",
    "                              terminal, np.reshape(s2, (qnet.state_dim,)))\n",
    "\n",
    "            # Keep adding experience to the memory until\n",
    "            # there are at least minibatch size samples\n",
    "            if replay_buffer.size() > MINIBATCH_SIZE:\n",
    "                s_batch, a_batch, r_batch, t_batch, s2_batch = replay_buffer.sample_batch(MINIBATCH_SIZE)\n",
    "\n",
    "                # Calculate targets\n",
    "                target_q = qnet.predect_q(s2_batch)\n",
    "\n",
    "                y_i = []\n",
    "                for k in range(MINIBATCH_SIZE):\n",
    "                    if t_batch[k]:\n",
    "                        y_i.append(r_batch[k])\n",
    "                    else:\n",
    "                        y_i.append(r_batch[k] + GAMMA * np.amax(target_q[k]))\n",
    "\n",
    "                # Update the critic given the targets\n",
    "                predicted_q_value, _ = qnet.train(s_batch, a_batch, np.reshape(y_i, (MINIBATCH_SIZE, 1)), num_epi)\n",
    "\n",
    "                ep_ave_max_q += np.amax(predicted_q_value)\n",
    "                \n",
    "                # Update target networks\n",
    "                qnet.update_target()\n",
    "\n",
    "            s = s2\n",
    "            ep_reward += r\n",
    "\n",
    "            if terminal or j == MAX_EPISODE_LEN-1:\n",
    "                \n",
    "                if EXPLORATION_RATE > 0.05 and terminal:\n",
    "                    EXPLORATION_RATE = EXPLORATION_RATE*0.9\n",
    "\n",
    "                summary_str = sess.run(summary_ops, feed_dict={\n",
    "                    summary_vars[0]: ep_reward,\n",
    "                    summary_vars[1]: ep_ave_max_q / float(j),\n",
    "                    summary_vars[2]: EXPLORATION_RATE\n",
    "                })\n",
    "\n",
    "                writer.add_summary(summary_str, num_epi)\n",
    "                writer.flush()\n",
    "\n",
    "#                 print('| Reward: {:d} | Episode: {:d} | Qmax: {:.4f} | Exploration: {:.6f} '.format(int(ep_reward), \\\n",
    "#                         num_epi, (ep_ave_max_q / float(j)), EXPLORATION_RATE))\n",
    "                \n",
    "                f = open(\"stats.txt\", \"ab\")\n",
    "                f.write(\"| Reward: \" + str(int(ep_reward)) \n",
    "                        +\" | Episode: \" + str(num_epi) \n",
    "                        + \" | Qmax: \" + str(ep_ave_max_q / float(j)) \n",
    "                        + \" | Exploration: \" + str(EXPLORATION_RATE) + \"\\n\")\n",
    "                f.close()\n",
    "                \n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.0001\n",
    "GAMMA = 0.99\n",
    "TAU = 0.001\n",
    "BUFFER_SIZE = 10**6\n",
    "MINIBATCH_SIZE = 64\n",
    "RANDOM_SEED = 27\n",
    "MAX_EPISODES = 50000\n",
    "MAX_EPISODE_LEN = 2000\n",
    "SUMMARY_DIR = './results/tf_ddqn'\n",
    "EXPLORATION_RATE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gaoqitong/anaconda/envs/py27/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.36638797e-09   2.03085437e-09]]\n",
      "[[  1.13478400e-07   9.60307318e-08]]\n",
      "[[  2.34739289e-07   1.98230182e-07]]\n",
      "[[  1.20775155e-07   1.02052923e-07]]\n",
      "[[  2.45703148e-07   2.07384090e-07]]\n",
      "[[  3.73229398e-07   3.14865247e-07]]\n",
      "[[  5.04544403e-07   4.25513093e-07]]\n",
      "[[  6.40804728e-07   5.40314772e-07]]\n",
      "[[  5.46757974e-07   4.60936320e-07]]\n",
      "[[  4.61263596e-07   3.89028770e-07]]\n",
      "[[  3.81215074e-07   3.21851587e-07]]\n",
      "[[  3.09338219e-07   2.61771106e-07]]\n",
      "[[  4.60335059e-07   3.89255604e-07]]\n",
      "[[  4.01646588e-07   3.40035427e-07]]\n",
      "[[  7.57037526e-08   6.35989039e-08]]\n",
      "[[  3.68362159e-08   3.03568974e-08]]\n",
      "[[  8.04936491e-08   6.76565648e-08]]\n",
      "[[  3.81851457e-08   3.14174606e-08]]\n",
      "[[  8.35922123e-08   7.02757248e-08]]\n",
      "[[  3.92424049e-08   3.22570770e-08]]\n",
      "[[  5.05334512e-08   4.17296135e-08]]\n",
      "[[  6.46818705e-08   5.34109290e-08]]\n",
      "[[  7.87888084e-08   6.50576055e-08]]\n",
      "[[  6.33629398e-08   5.20976222e-08]]\n",
      "[[  4.72101007e-08   3.85818275e-08]]\n",
      "[[  3.04914067e-08   2.46402667e-08]]\n",
      "[[  4.24778186e-08   3.46286058e-08]]\n",
      "[[  2.51303209e-08   2.01780459e-08]]\n",
      "[[  1.31270363e-08   1.04854978e-08]]\n",
      "[[  6.11902777e-08   5.10367784e-08]]\n",
      "[[  8.21078849e-09   6.66084521e-09]]\n",
      "[[  1.41383278e-08   1.14448770e-08]]\n",
      "[[  5.72635717e-09   4.68010608e-09]]\n",
      "[[  7.61198304e-09   5.70653924e-09]]\n",
      "[[  6.78519214e-08   5.51384254e-08]]\n",
      "[[  2.00821221e-10   1.41297307e-10]]\n",
      "[[  4.22541468e-08   3.39890640e-08]]\n",
      "[[ 0.  0.]]\n",
      "[[  2.29703438e-08   1.82449202e-08]]\n",
      "[[  8.80715447e-08   7.15650614e-08]]\n",
      "[[  1.64027057e-07   1.34379803e-07]]\n",
      "[[  6.97692784e-08   5.65440992e-08]]\n",
      "[[  1.42732091e-07   1.16562774e-07]]\n",
      "[[  2.29235951e-07   1.88752310e-07]]\n",
      "[[  3.29410824e-07   2.73613097e-07]]\n",
      "[[  4.35096126e-07   3.63010571e-07]]\n",
      "[[  5.48896480e-07   4.58904793e-07]]\n",
      "[[  6.71883811e-07   5.62219213e-07]]\n",
      "[[  8.02682109e-07   6.72118460e-07]]\n",
      "[[  9.42123165e-07   7.89307762e-07]]\n",
      "[[  1.09385144e-06   9.17065904e-07]]\n",
      "[[  5.30132773e-08   4.43498713e-08]]\n",
      "[[  3.43956543e-08   2.84585688e-08]]\n",
      "[[  5.86215094e-08   4.91168706e-08]]\n",
      "[[  1.68177493e-07   1.41725607e-07]]\n",
      "[[  2.87124266e-07   2.41837569e-07]]\n",
      "[[  4.10856785e-07   3.46063814e-07]]\n",
      "[[  5.38606457e-07   4.53680940e-07]]\n",
      "[[  6.71607268e-07   5.65739299e-07]]\n",
      "[[  8.11037069e-07   6.83241069e-07]]\n",
      "[[  9.57986686e-07   8.07114361e-07]]\n",
      "[[  8.82415748e-07   7.43602925e-07]]\n",
      "[[  6.49435061e-10   5.71566849e-10]]\n",
      "[[  1.09154463e-07   9.15743499e-08]]\n",
      "[[  2.35743514e-07   1.98338341e-07]]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'QNet' object has no attribute 'predect_q'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c822431161c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mQnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTAU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMINIBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./saved_model/ddqn.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-60ec35d2b583>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(sess, env, qnet)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0;31m# Calculate targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0mtarget_q\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredect_q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms2_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0my_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'QNet' object has no attribute 'predect_q'"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    env = gym.make('CartPole-v0')\n",
    "    env.render()\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    tf.set_random_seed(RANDOM_SEED)\n",
    "    env.seed(RANDOM_SEED)\n",
    "    \n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    Qnet = QNet(sess, state_dim, action_dim, LEARNING_RATE, TAU, MINIBATCH_SIZE, \"./saved_model/ddqn.ckpt\")\n",
    "    \n",
    "    train(sess, env, Qnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "world = np.zeros(env.shape)\n",
    "a_list = []\n",
    "s_list = []\n",
    "for s in range(env.nS):\n",
    "    a_list += [np.argmax(P[s])]\n",
    "    s_list += [np.unravel_index(s,env.shape)]\n",
    "for s,a in zip(s_list,a_list):\n",
    "    world[s] = a\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%matplotlib auto\n",
    "plt.imshow(world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "plotting.plot_episode_stats(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def get_optimal_path(Q,env):\n",
    "    env.reset()\n",
    "    start_state = env.start_state\n",
    "    terminal_state = env.terminal_state\n",
    "    state = np.ravel_multi_index(start_state,env.shape)\n",
    "    path = [start_state]\n",
    "    value = 0\n",
    "    action = []\n",
    "    while 1:\n",
    "        next_action = np.argmax(Q[state])\n",
    "        next_state,reward,done,_ = env.step(next_action)\n",
    "        path += [np.unravel_index(next_state,env.shape)]\n",
    "        value += reward\n",
    "        action += [next_action]\n",
    "        if done:\n",
    "            return path, action, value\n",
    "            break\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "opt_path,action,value = get_optimal_path(Q,env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "%matplotlib auto\n",
    "world = deepcopy(env.winds)\n",
    "t = 0\n",
    "for i in opt_path[:-1]:\n",
    "    world[i] = 6\n",
    "#     world[i] += action[t]\n",
    "    t+=1\n",
    "plt.imshow(world)\n",
    "# print value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
